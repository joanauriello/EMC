{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- SOLAMENTE COLUMNAS NUMERICAS, SI TENES CATEGORICAS O STRINGS DROPEALAS\n",
        "- Si dunn_index no está instalada, puedes instalarla con pip install dunn-index.\n",
        "- Medidas de distancia: Cambia entre 'euclidean', 'manhattan', etc., según el método de clustering."
      ],
      "metadata": {
        "id": "FP2NzNl4Gvgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8gmf4wRGPJs"
      },
      "outputs": [],
      "source": [
        "#metodo del codo para definicion de K\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def metodo_codo(datos, max_k=10):\n",
        "    \"\"\"\n",
        "    Calcula la suma de cuadrados within (inercia) para varios valores de K.\n",
        "    \"\"\"\n",
        "    inercia = []\n",
        "    for k in range(1, max_k + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(datos)\n",
        "        inercia.append(kmeans.inertia_)\n",
        "\n",
        "    # Gráfico del método del codo\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, max_k + 1), inercia, marker='o')\n",
        "    plt.xlabel('Número de Clusters (K)')\n",
        "    plt.ylabel('Suma de Cuadrados Within (Inercia)')\n",
        "    plt.title('Método del Codo para Definir K')\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aplicacion KMEANS\n",
        "def aplicar_kmeans(datos, n_clusters):\n",
        "    \"\"\"\n",
        "    Aplica el algoritmo de K-Means y muestra los clusters y sus valores.\n",
        "    \"\"\"\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(datos)\n",
        "\n",
        "    # Mostrar los clusters\n",
        "    print(\"\\nClusters asignados a cada observación:\")\n",
        "    print(pd.Series(clusters).value_counts().sort_index())\n",
        "\n",
        "    # Retornar los clusters y el modelo\n",
        "    return clusters, kmeans\n"
      ],
      "metadata": {
        "id": "xC8TczlaGWdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VISUALIZACION CLUSTERS EN GRAFICO\n",
        "def visualizar_clusters_pca(datos, clusters):\n",
        "    \"\"\"\n",
        "    Reduce los datos a 2D usando PCA y visualiza los clusters.\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=2)\n",
        "    datos_pca = pca.fit_transform(datos)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(datos_pca[:, 0], datos_pca[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
        "    plt.xlabel('Componente Principal 1')\n",
        "    plt.ylabel('Componente Principal 2')\n",
        "    plt.title('Visualización de Clusters en 2D (PCA)')\n",
        "    plt.colorbar(label='Cluster')\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "0uQqZtP7GcJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APLICACION PAM CON DISTANCIA DE MANHATTAN\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "def aplicar_pam(datos, n_clusters):\n",
        "    \"\"\"\n",
        "    Aplica PAM (K-Medoids) con distancia de Manhattan.\n",
        "    \"\"\"\n",
        "    pam = KMedoids(n_clusters=n_clusters, metric='manhattan', random_state=42)\n",
        "    clusters = pam.fit_predict(datos)\n",
        "\n",
        "    print(\"\\nClusters asignados a cada observación (PAM):\")\n",
        "    print(pd.Series(clusters).value_counts().sort_index())\n",
        "\n",
        "    return clusters, pam\n"
      ],
      "metadata": {
        "id": "775i9KPiGgha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAFICO DE SILOHUETTE Y DUNN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from dunn_index import dunn\n",
        "\n",
        "def evaluar_clusters(datos, clusters, metric='euclidean'):\n",
        "    \"\"\"\n",
        "    Calcula el puntaje de Silhouette y Dunn para los clusters.\n",
        "    \"\"\"\n",
        "    silhouette = silhouette_score(datos, clusters, metric=metric)\n",
        "    print(f\"Puntaje de Silhouette: {silhouette:.4f}\")\n",
        "\n",
        "    dunn_score = dunn(pairwise_distances(datos, metric=metric), clusters)\n",
        "    print(f\"Índice de Dunn: {dunn_score:.4f}\")\n",
        "\n",
        "    return silhouette, dunn_score\n"
      ],
      "metadata": {
        "id": "k40GT6GzGiJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COMPARACION DE METODOS : RAND Y JACCARD\n",
        "from sklearn.metrics import adjusted_rand_score, jaccard_score\n",
        "\n",
        "def comparar_metodos(clusters1, clusters2):\n",
        "    \"\"\"\n",
        "    Compara dos métodos de clustering usando Rand y Jaccard.\n",
        "    \"\"\"\n",
        "    rand_index = adjusted_rand_score(clusters1, clusters2)\n",
        "    jaccard_index = jaccard_score(clusters1, clusters2, average='macro')\n",
        "\n",
        "    print(f\"Índice de Rand: {rand_index:.4f}\")\n",
        "    print(f\"Índice de Jaccard: {jaccard_index:.4f}\")\n",
        "\n",
        "    return rand_index, jaccard_index\n"
      ],
      "metadata": {
        "id": "7_XgSUOEGl0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CLUSTERING JERARQUICO Y COEF DE COHPENETIC\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "def clustering_jerarquico(datos, metodo):\n",
        "    \"\"\"\n",
        "    Aplica clustering jerárquico y calcula el coeficiente cophenético.\n",
        "    \"\"\"\n",
        "    Z = linkage(datos, method=metodo)\n",
        "    coph_corr, _ = cophenet(Z, pdist(datos))\n",
        "\n",
        "    print(f\"Coeficiente cophenético ({metodo}): {coph_corr:.4f}\")\n",
        "\n",
        "    # Generar dendrograma\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    dendrogram(Z)\n",
        "    plt.title(f'Dendrograma (Método: {metodo})')\n",
        "    plt.xlabel('Observaciones')\n",
        "    plt.ylabel('Distancia')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return Z, coph_corr\n"
      ],
      "metadata": {
        "id": "WaIQDApnGqej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adicional: INDICE DE HOPKINS PARA TENDENCIA\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def calcular_hopkins(X, n_samples=100):\n",
        "    \"\"\"\n",
        "    Calcula el coeficiente de Hopkins para un dataset.\n",
        "    X: Dataset (matriz de datos numéricos).\n",
        "    n_samples: Número de muestras aleatorias a usar para el cálculo.\n",
        "    Retorna:\n",
        "    - Coeficiente de Hopkins.\n",
        "    \"\"\"\n",
        "    # Verificar que el dataset sea suficientemente grande\n",
        "    if X.shape[0] < n_samples:\n",
        "        n_samples = X.shape[0] // 2\n",
        "\n",
        "    # Seleccionar n_samples aleatorios de los datos\n",
        "    np.random.seed(42)\n",
        "    idx = np.random.choice(range(X.shape[0]), n_samples, replace=False)\n",
        "    X_sample = X[idx]\n",
        "\n",
        "    # Generar n_samples puntos aleatorios dentro del rango de los datos\n",
        "    X_min, X_max = np.min(X, axis=0), np.max(X, axis=0)\n",
        "    X_random = np.random.uniform(X_min, X_max, (n_samples, X.shape[1]))\n",
        "\n",
        "    # Calcular las distancias al vecino más cercano en el dataset original\n",
        "    nn = NearestNeighbors(n_neighbors=1).fit(X)\n",
        "    distances_sample, _ = nn.kneighbors(X_sample, n_neighbors=1)\n",
        "    distances_random, _ = nn.kneighbors(X_random, n_neighbors=1)\n",
        "\n",
        "    # Calcular el coeficiente de Hopkins\n",
        "    W = np.sum(distances_sample)\n",
        "    U = np.sum(distances_random)\n",
        "    H = W / (W + U)\n",
        "\n",
        "    return H\n",
        "\n",
        "# Ejemplo de uso\n",
        "# df_numerico = df.select_dtypes(include=[np.number])  # Asegúrate de usar solo columnas numéricas\n",
        "# hopkins = calcular_hopkins(df_numerico.values, n_samples=100)\n",
        "# print(f\"Coeficiente de Hopkins: {hopkins:.4f}\")\n"
      ],
      "metadata": {
        "id": "v8dMbeLlHFuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrate de usar solo columnas numéricas\n",
        "df_numerico = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calcular el coeficiente de Hopkins\n",
        "hopkins = calcular_hopkins(df_numerico.values, n_samples=100)\n",
        "print(f\"Coeficiente de Hopkins: {hopkins:.4f}\")\n",
        "\n",
        "# Si el coeficiente es cercano a 1, procede con clustering\n",
        "if hopkins > 0.75:\n",
        "    print(\"Los datos tienen estructura de clustering. Continuando con el análisis...\")\n",
        "    metodo_codo(df_numerico, max_k=10)\n",
        "else:\n",
        "    print(\"Los datos no tienen una estructura clara de clustering. Revisa el dataset.\")\n"
      ],
      "metadata": {
        "id": "9x_Tbs_eHREn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}